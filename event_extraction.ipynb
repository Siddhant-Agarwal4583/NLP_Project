{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xu_is53zj_au",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed133de3-22b8-4e73-fa99-738f46d5cff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from newsapi-python) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip3 install newsapi-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"newsapi-python\" package is a wrapper for the News API, which is a service that provides access to news articles from various sources around the world. By installing this package, developers can use Python to interact with the News API and retrieve news articles based on specific criteria such as keywords, sources, and time periods."
      ],
      "metadata": {
        "id": "QHAtrBCkm-W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from newsapi import NewsApiClient\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "wVRl59Z0lIgT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports several Python packages:\n",
        "\n",
        "- tqdm: A package that provides progress bars for long-running loops or tasks.\n",
        "- pandas: A package for data manipulation and analysis.\n",
        "- numpy: A package for numerical computing in Python.\n",
        "- newsapi: A package for accessing news articles from various sources around the world.\n",
        "- nltk: The Natural Language Toolkit, a package for working with human language data in Python.\n",
        "- spacy: A package for natural language processing in Python.\n",
        "- sklearn: The Scikit-learn package, a machine learning library for Python.\n",
        "- matplotlib: A plotting library for Python.\n",
        "seaborn: A data visualization library based on Matplotlib."
      ],
      "metadata": {
        "id": "NHkAF2ubngzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '946472f8ad574082a31c9d0fcaf1e89d'\n",
        "newsapi = NewsApiClient(api_key=api_key)"
      ],
      "metadata": {
        "id": "cHgPEmW0lOQc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first line assigns a string value to the variable api_key. This value is an authentication key that is required to access the News API. It is a unique identifier that is associated with a specific user account and allows the user to make requests to the API.\n",
        "\n",
        "The second line creates an instance of the NewsApiClient class and assigns it to the variable newsapi. The NewsApiClient class is defined in the newsapi package and provides a set of methods that allow developers to interact with the News API.\n",
        "\n",
        "The api_key argument is passed to the NewsApiClient constructor to authenticate the client and enable it to make requests to the API on behalf of the user account associated with the provided API key. Once this instance of the NewsApiClient class is created, it can be used to call various methods that retrieve news articles from the API based on specific criteria such as keywords, sources, and time periods."
      ],
      "metadata": {
        "id": "TrCKiZXgoA0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_news(query):\n",
        "    all_results = []\n",
        "    for pag in tqdm(range(1, 6)):\n",
        "        pag_articles = newsapi.get_everything(q=query, sort_by='relevancy', page=pag)['articles']\n",
        "        if len(pag_articles) == 0:\n",
        "            break\n",
        "        all_results.extend(pag_articles)\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "SoddHCbYlSln"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Python function that uses the newsapi package to crawl news articles from various sources based on a search query.\n",
        "\n",
        "The function takes a single argument, query, which is a string representing the search term or topic of interest.\n",
        "\n",
        "The for loop iterates over 5 pages of news articles (i.e., range(1,6)). For each page, the newsapi.get_everything() method is called with the q argument set to the query parameter and the sort_by argument set to 'relevancy'. This retrieves news articles from the News API that match the search query and sorts them by relevance. The page argument is set to the current page number in the loop, which allows the function to retrieve news articles from multiple pages of search results.\n",
        "\n",
        "The retrieved news articles are stored in the pag_articles variable as a list of dictionaries, with each dictionary representing an article and containing various metadata such as the article's title, author, source, publication date, and content.\n",
        "\n",
        "If the length of pag_articles is zero, the loop breaks, as this indicates that there are no more pages of search results to retrieve.\n",
        "\n",
        "Finally, the list of all retrieved news articles across all pages is returned as the output of the function.\n",
        "\n",
        "The function uses the tqdm package to display a progress bar during the loop, indicating the percentage of pages that have been crawled so far."
      ],
      "metadata": {
        "id": "6Njsly3lpHO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tesla_news = crawl_news('tesla')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ADci2alUra",
        "outputId": "646956cf-a16f-48f1-e74f-f89fb790466b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:03<00:00,  1.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('BBC news dataset.csv', usecols=range(1, 3))"
      ],
      "metadata": {
        "id": "N9c_zbyRlXuR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code reads a CSV file called 'BBC news dataset.csv' using the pandas package, and creates a DataFrame object named df that contains the data from the CSV file.\n",
        "\n",
        "The usecols parameter is used to select only specific columns from the CSV file. In this case, the range(1, 3) argument selects columns 1 and 2, which correspond to the 'description' and 'tags' columns of the BBC news dataset."
      ],
      "metadata": {
        "id": "5eRK00U8prLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates('description', inplace=True)"
      ],
      "metadata": {
        "id": "moRaM-NMlaIs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code drops any duplicate rows in the DataFrame object df based on the values in the 'description' column, and modifies df in place by setting the inplace parameter to True.\n",
        "\n",
        "The drop_duplicates() method is a built-in method of the pandas package that removes duplicate rows from a DataFrame. The 'description' column is used as the key to identify duplicates. If there are any rows with identical values in the 'description' column, only the first occurrence of such a row is kept and all subsequent duplicates are removed.\n",
        "\n",
        "By setting inplace=True, the drop_duplicates() method modifies the DataFrame object df directly, without creating a new object. This means that any subsequent operations on df will reflect the changes made by this method."
      ],
      "metadata": {
        "id": "FEHXK08FqaFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = df['description'].values"
      ],
      "metadata": {
        "id": "06AWCS4BlcgL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "QVT3BCQ_le0o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a function called pre_process_text that takes a string of text as its input, performs several text preprocessing steps, and returns a list of preprocessed words.\n",
        "\n",
        "The text preprocessing steps are as follows:\n",
        "\n",
        "1. Tokenization: The input text is split into individual words or tokens using the word_tokenize() function from the nltk package.\n",
        "2. Lowercasing: All words are converted to lowercase using a list comprehension.\n",
        "3. Removing punctuation: Punctuation marks are removed from each word using the string.punctuation string and the str.translate() method.\n",
        "4. Removing non-alphabetic characters: Words that contain non-alphabetic characters (such as numbers or symbols) are removed using a list comprehension and the str.isalpha() method.\n",
        "5. Removing stop words: Common stop words (such as \"the\", \"and\", \"a\") are removed using the stopwords.words() method from the nltk package and another list comprehension.\n",
        "\n",
        "The preprocessed words are returned as a list.\n",
        "\n",
        "This function can be used to preprocess the descriptions of news articles in the BBC news dataset before clustering or visualization."
      ],
      "metadata": {
        "id": "uYh4HTgJrMGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFLezKNOlwa6",
        "outputId": "52f428fb-aafa-412d-9a0e-5d5cfbad18ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The punkt and stopwords datasets are required for tokenization and stopword removal, respectively."
      ],
      "metadata": {
        "id": "5hksBGTWrqVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_descriptions = []\n",
        "for description in tqdm(descriptions):\n",
        "    processed_descriptions.append(' '.join(pre_process_text(description)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EeEw2H_lhhG",
        "outputId": "15d4a161-14bb-4522-fe4d-8237462e8fe3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2128/2128 [00:05<00:00, 413.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a loop that iterates over each description in the descriptions numpy array, applies the pre_process_text() function to each description, and appends the preprocessed description to a new list called processed_descriptions.\n",
        "\n",
        "The tqdm() function is used to display a progress bar that indicates how far along the loop is in processing the descriptions.\n",
        "\n",
        "Inside the loop, the pre_process_text() function is applied to each description. The resulting list of preprocessed words is joined into a single string using the join() method with a space separator. The joined string is then appended to the processed_descriptions list.\n",
        "\n",
        "After the loop is finished, the processed_descriptions list contains all of the preprocessed descriptions of the news articles in the BBC news dataset. Each preprocessed description is a single string containing lowercase words with no punctuation, non-alphabetic characters, or stop words."
      ],
      "metadata": {
        "id": "EgvgN8N0sfoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sent_vecs = {}\n",
        "docs = []\n",
        "\n",
        "for index, description in enumerate(tqdm(processed_descriptions)):\n",
        "    doc = nlp(description)\n",
        "    docs.append(doc)\n",
        "    sent_vecs[index] = doc.vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bdf6XJTl5jY",
        "outputId": "b06288be-f74b-422c-96e7-e11bdbd80bd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2128/2128 [01:39<00:00, 21.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a block of code that uses the spacy package to compute the vector representation of each preprocessed description in the processed_descriptions list.\n",
        "\n",
        "The first line loads a pre-trained model called en_core_web_sm from the spacy package. This model includes word vectors and linguistic annotations for the English language.\n",
        "\n",
        "The sent_vecs dictionary is initialized to an empty dictionary, and the docs list is initialized to an empty list.\n",
        "\n",
        "The code then loops through each preprocessed description in processed_descriptions. For each description, spacy is used to create a document object doc containing linguistic annotations and vectors for each word in the description.\n",
        "\n",
        "The document object doc is added to the docs list. The vector representation of the entire document is extracted from doc using the doc.vector attribute, and is stored in the sent_vecs dictionary with the index of the description as the key.\n",
        "\n",
        "After the loop is finished, the sent_vecs dictionary contains a mapping of each index to the vector representation of the corresponding preprocessed description in processed_descriptions. The docs list contains a spacy document object for each preprocessed description."
      ],
      "metadata": {
        "id": "fvBCSfIEtdfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = list(sent_vecs.values())"
      ],
      "metadata": {
        "id": "Bv5dUfZVl9eI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code extracts the vector representations of the preprocessed descriptions from the sent_vecs dictionary and converts them to a list.\n",
        "\n",
        "The sent_vecs dictionary maps the index of each preprocessed description to its corresponding vector representation. The values() method of a dictionary returns a list of all the values in the dictionary, in this case a list of all the vector representations of the preprocessed descriptions.\n",
        "\n",
        "The list() function is used to convert the values() view object to a list. The resulting vectors list contains the vector representations of all the preprocessed descriptions in the BBC news dataset."
      ],
      "metadata": {
        "id": "2opY4I5ot0Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = np.array(vectors)"
      ],
      "metadata": {
        "id": "PlGAW7U6l_IQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_results = {}\n",
        "for i in tqdm(np.arange(0.001, 1, 0.002)):\n",
        "    dbscan = DBSCAN(eps=i, min_samples=3, metric='cosine').fit(vectors)\n",
        "    labels_results[i] = len(pd.Series(dbscan.labels_).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8g7UuFkmBCQ",
        "outputId": "32ab69dc-41ec-4db2-bc22-83a8855f6521"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a block of code that performs density-based clustering on the vector representations of the preprocessed descriptions using the DBSCAN algorithm from the sklearn.cluster module.\n",
        "\n",
        "The algorithm is run with varying values of the eps parameter, which controls the maximum distance between two points for them to be considered part of the same cluster.\n",
        "\n",
        "The labels_results dictionary is initialized to an empty dictionary. The code then loops through a range of values between 0.001 and 1 in increments of 0.002 using np.arange().\n",
        "\n",
        "For each value of eps, DBSCAN is run on the vector representations in vectors with the specified value of eps, min_samples=5 (which sets the minimum number of points required to form a dense region), and metric='cosine' (which specifies the distance metric to be used for measuring the similarity between the vectors).\n",
        "\n",
        "The resulting labels from DBSCAN are stored in a labels_ attribute. The number of unique labels is counted using pd.Series(dbscan.labels_).value_counts() and stored in the labels_results dictionary with the current value of eps as the key.\n",
        "\n",
        "After the loop is finished, the labels_results dictionary contains a mapping of each value of eps to the number of unique labels produced by DBSCAN using that value of eps. This information can be used to determine the optimal value of eps to use for clustering."
      ],
      "metadata": {
        "id": "Vl6MB36Ju6oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(0.001, 1, 0.002):\n",
        "    print('{}: {}'.format(i, labels_results[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvyKpFLBmC6_",
        "outputId": "8572b8b7-761c-4c27-f505-9a3b94d0bca4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001: 1\n",
            "0.003: 1\n",
            "0.005: 1\n",
            "0.007: 1\n",
            "0.009000000000000001: 4\n",
            "0.011: 12\n",
            "0.013000000000000001: 20\n",
            "0.015: 22\n",
            "0.017: 7\n",
            "0.019000000000000003: 4\n",
            "0.021: 6\n",
            "0.023: 4\n",
            "0.025: 4\n",
            "0.027000000000000003: 4\n",
            "0.029: 3\n",
            "0.031: 2\n",
            "0.033: 2\n",
            "0.035: 2\n",
            "0.037000000000000005: 2\n",
            "0.039: 2\n",
            "0.041: 2\n",
            "0.043000000000000003: 2\n",
            "0.045: 2\n",
            "0.047: 2\n",
            "0.049: 2\n",
            "0.051000000000000004: 2\n",
            "0.053000000000000005: 2\n",
            "0.055: 2\n",
            "0.057: 2\n",
            "0.059000000000000004: 2\n",
            "0.061: 2\n",
            "0.063: 2\n",
            "0.065: 2\n",
            "0.067: 2\n",
            "0.069: 2\n",
            "0.07100000000000001: 2\n",
            "0.07300000000000001: 2\n",
            "0.075: 2\n",
            "0.077: 2\n",
            "0.079: 2\n",
            "0.081: 2\n",
            "0.083: 2\n",
            "0.085: 2\n",
            "0.08700000000000001: 2\n",
            "0.089: 2\n",
            "0.091: 2\n",
            "0.093: 2\n",
            "0.095: 2\n",
            "0.097: 2\n",
            "0.099: 2\n",
            "0.101: 2\n",
            "0.10300000000000001: 2\n",
            "0.10500000000000001: 2\n",
            "0.107: 2\n",
            "0.109: 2\n",
            "0.111: 2\n",
            "0.113: 2\n",
            "0.115: 2\n",
            "0.117: 2\n",
            "0.11900000000000001: 2\n",
            "0.121: 2\n",
            "0.123: 2\n",
            "0.125: 2\n",
            "0.127: 2\n",
            "0.129: 2\n",
            "0.131: 2\n",
            "0.133: 2\n",
            "0.135: 2\n",
            "0.137: 2\n",
            "0.139: 2\n",
            "0.14100000000000001: 2\n",
            "0.14300000000000002: 2\n",
            "0.14500000000000002: 2\n",
            "0.147: 2\n",
            "0.149: 2\n",
            "0.151: 2\n",
            "0.153: 2\n",
            "0.155: 2\n",
            "0.157: 2\n",
            "0.159: 2\n",
            "0.161: 2\n",
            "0.163: 2\n",
            "0.165: 2\n",
            "0.167: 2\n",
            "0.169: 2\n",
            "0.171: 2\n",
            "0.17300000000000001: 2\n",
            "0.17500000000000002: 2\n",
            "0.177: 2\n",
            "0.179: 2\n",
            "0.181: 2\n",
            "0.183: 2\n",
            "0.185: 2\n",
            "0.187: 2\n",
            "0.189: 2\n",
            "0.191: 2\n",
            "0.193: 2\n",
            "0.195: 2\n",
            "0.197: 2\n",
            "0.199: 2\n",
            "0.201: 2\n",
            "0.203: 2\n",
            "0.20500000000000002: 2\n",
            "0.20700000000000002: 2\n",
            "0.20900000000000002: 2\n",
            "0.211: 2\n",
            "0.213: 2\n",
            "0.215: 2\n",
            "0.217: 2\n",
            "0.219: 2\n",
            "0.221: 2\n",
            "0.223: 2\n",
            "0.225: 2\n",
            "0.227: 2\n",
            "0.229: 2\n",
            "0.231: 2\n",
            "0.233: 1\n",
            "0.23500000000000001: 1\n",
            "0.23700000000000002: 1\n",
            "0.23900000000000002: 1\n",
            "0.241: 1\n",
            "0.243: 1\n",
            "0.245: 1\n",
            "0.247: 1\n",
            "0.249: 1\n",
            "0.251: 1\n",
            "0.253: 1\n",
            "0.255: 1\n",
            "0.257: 1\n",
            "0.259: 1\n",
            "0.261: 1\n",
            "0.263: 1\n",
            "0.265: 1\n",
            "0.267: 1\n",
            "0.269: 1\n",
            "0.271: 1\n",
            "0.273: 1\n",
            "0.275: 1\n",
            "0.277: 1\n",
            "0.279: 1\n",
            "0.281: 1\n",
            "0.28300000000000003: 1\n",
            "0.28500000000000003: 1\n",
            "0.28700000000000003: 1\n",
            "0.28900000000000003: 1\n",
            "0.291: 1\n",
            "0.293: 1\n",
            "0.295: 1\n",
            "0.297: 1\n",
            "0.299: 1\n",
            "0.301: 1\n",
            "0.303: 1\n",
            "0.305: 1\n",
            "0.307: 1\n",
            "0.309: 1\n",
            "0.311: 1\n",
            "0.313: 1\n",
            "0.315: 1\n",
            "0.317: 1\n",
            "0.319: 1\n",
            "0.321: 1\n",
            "0.323: 1\n",
            "0.325: 1\n",
            "0.327: 1\n",
            "0.329: 1\n",
            "0.331: 1\n",
            "0.333: 1\n",
            "0.335: 1\n",
            "0.337: 1\n",
            "0.339: 1\n",
            "0.341: 1\n",
            "0.343: 1\n",
            "0.34500000000000003: 1\n",
            "0.34700000000000003: 1\n",
            "0.34900000000000003: 1\n",
            "0.35100000000000003: 1\n",
            "0.353: 1\n",
            "0.355: 1\n",
            "0.357: 1\n",
            "0.359: 1\n",
            "0.361: 1\n",
            "0.363: 1\n",
            "0.365: 1\n",
            "0.367: 1\n",
            "0.369: 1\n",
            "0.371: 1\n",
            "0.373: 1\n",
            "0.375: 1\n",
            "0.377: 1\n",
            "0.379: 1\n",
            "0.381: 1\n",
            "0.383: 1\n",
            "0.385: 1\n",
            "0.387: 1\n",
            "0.389: 1\n",
            "0.391: 1\n",
            "0.393: 1\n",
            "0.395: 1\n",
            "0.397: 1\n",
            "0.399: 1\n",
            "0.401: 1\n",
            "0.403: 1\n",
            "0.405: 1\n",
            "0.40700000000000003: 1\n",
            "0.40900000000000003: 1\n",
            "0.41100000000000003: 1\n",
            "0.41300000000000003: 1\n",
            "0.41500000000000004: 1\n",
            "0.41700000000000004: 1\n",
            "0.419: 1\n",
            "0.421: 1\n",
            "0.423: 1\n",
            "0.425: 1\n",
            "0.427: 1\n",
            "0.429: 1\n",
            "0.431: 1\n",
            "0.433: 1\n",
            "0.435: 1\n",
            "0.437: 1\n",
            "0.439: 1\n",
            "0.441: 1\n",
            "0.443: 1\n",
            "0.445: 1\n",
            "0.447: 1\n",
            "0.449: 1\n",
            "0.451: 1\n",
            "0.453: 1\n",
            "0.455: 1\n",
            "0.457: 1\n",
            "0.459: 1\n",
            "0.461: 1\n",
            "0.463: 1\n",
            "0.465: 1\n",
            "0.467: 1\n",
            "0.46900000000000003: 1\n",
            "0.47100000000000003: 1\n",
            "0.47300000000000003: 1\n",
            "0.47500000000000003: 1\n",
            "0.47700000000000004: 1\n",
            "0.47900000000000004: 1\n",
            "0.481: 1\n",
            "0.483: 1\n",
            "0.485: 1\n",
            "0.487: 1\n",
            "0.489: 1\n",
            "0.491: 1\n",
            "0.493: 1\n",
            "0.495: 1\n",
            "0.497: 1\n",
            "0.499: 1\n",
            "0.501: 1\n",
            "0.503: 1\n",
            "0.505: 1\n",
            "0.507: 1\n",
            "0.509: 1\n",
            "0.511: 1\n",
            "0.513: 1\n",
            "0.515: 1\n",
            "0.517: 1\n",
            "0.519: 1\n",
            "0.521: 1\n",
            "0.523: 1\n",
            "0.525: 1\n",
            "0.527: 1\n",
            "0.529: 1\n",
            "0.531: 1\n",
            "0.533: 1\n",
            "0.535: 1\n",
            "0.537: 1\n",
            "0.539: 1\n",
            "0.541: 1\n",
            "0.543: 1\n",
            "0.545: 1\n",
            "0.547: 1\n",
            "0.549: 1\n",
            "0.551: 1\n",
            "0.553: 1\n",
            "0.555: 1\n",
            "0.557: 1\n",
            "0.559: 1\n",
            "0.561: 1\n",
            "0.5630000000000001: 1\n",
            "0.5650000000000001: 1\n",
            "0.5670000000000001: 1\n",
            "0.5690000000000001: 1\n",
            "0.5710000000000001: 1\n",
            "0.5730000000000001: 1\n",
            "0.5750000000000001: 1\n",
            "0.5770000000000001: 1\n",
            "0.579: 1\n",
            "0.581: 1\n",
            "0.583: 1\n",
            "0.585: 1\n",
            "0.587: 1\n",
            "0.589: 1\n",
            "0.591: 1\n",
            "0.593: 1\n",
            "0.595: 1\n",
            "0.597: 1\n",
            "0.599: 1\n",
            "0.601: 1\n",
            "0.603: 1\n",
            "0.605: 1\n",
            "0.607: 1\n",
            "0.609: 1\n",
            "0.611: 1\n",
            "0.613: 1\n",
            "0.615: 1\n",
            "0.617: 1\n",
            "0.619: 1\n",
            "0.621: 1\n",
            "0.623: 1\n",
            "0.625: 1\n",
            "0.627: 1\n",
            "0.629: 1\n",
            "0.631: 1\n",
            "0.633: 1\n",
            "0.635: 1\n",
            "0.637: 1\n",
            "0.639: 1\n",
            "0.641: 1\n",
            "0.643: 1\n",
            "0.645: 1\n",
            "0.647: 1\n",
            "0.649: 1\n",
            "0.651: 1\n",
            "0.653: 1\n",
            "0.655: 1\n",
            "0.657: 1\n",
            "0.659: 1\n",
            "0.661: 1\n",
            "0.663: 1\n",
            "0.665: 1\n",
            "0.667: 1\n",
            "0.669: 1\n",
            "0.671: 1\n",
            "0.673: 1\n",
            "0.675: 1\n",
            "0.677: 1\n",
            "0.679: 1\n",
            "0.681: 1\n",
            "0.683: 1\n",
            "0.685: 1\n",
            "0.687: 1\n",
            "0.6890000000000001: 1\n",
            "0.6910000000000001: 1\n",
            "0.6930000000000001: 1\n",
            "0.6950000000000001: 1\n",
            "0.6970000000000001: 1\n",
            "0.6990000000000001: 1\n",
            "0.7010000000000001: 1\n",
            "0.7030000000000001: 1\n",
            "0.705: 1\n",
            "0.707: 1\n",
            "0.709: 1\n",
            "0.711: 1\n",
            "0.713: 1\n",
            "0.715: 1\n",
            "0.717: 1\n",
            "0.719: 1\n",
            "0.721: 1\n",
            "0.723: 1\n",
            "0.725: 1\n",
            "0.727: 1\n",
            "0.729: 1\n",
            "0.731: 1\n",
            "0.733: 1\n",
            "0.735: 1\n",
            "0.737: 1\n",
            "0.739: 1\n",
            "0.741: 1\n",
            "0.743: 1\n",
            "0.745: 1\n",
            "0.747: 1\n",
            "0.749: 1\n",
            "0.751: 1\n",
            "0.753: 1\n",
            "0.755: 1\n",
            "0.757: 1\n",
            "0.759: 1\n",
            "0.761: 1\n",
            "0.763: 1\n",
            "0.765: 1\n",
            "0.767: 1\n",
            "0.769: 1\n",
            "0.771: 1\n",
            "0.773: 1\n",
            "0.775: 1\n",
            "0.777: 1\n",
            "0.779: 1\n",
            "0.781: 1\n",
            "0.783: 1\n",
            "0.785: 1\n",
            "0.787: 1\n",
            "0.789: 1\n",
            "0.791: 1\n",
            "0.793: 1\n",
            "0.795: 1\n",
            "0.797: 1\n",
            "0.799: 1\n",
            "0.801: 1\n",
            "0.803: 1\n",
            "0.805: 1\n",
            "0.807: 1\n",
            "0.809: 1\n",
            "0.811: 1\n",
            "0.8130000000000001: 1\n",
            "0.8150000000000001: 1\n",
            "0.8170000000000001: 1\n",
            "0.8190000000000001: 1\n",
            "0.8210000000000001: 1\n",
            "0.8230000000000001: 1\n",
            "0.8250000000000001: 1\n",
            "0.8270000000000001: 1\n",
            "0.8290000000000001: 1\n",
            "0.8310000000000001: 1\n",
            "0.8330000000000001: 1\n",
            "0.835: 1\n",
            "0.837: 1\n",
            "0.839: 1\n",
            "0.841: 1\n",
            "0.843: 1\n",
            "0.845: 1\n",
            "0.847: 1\n",
            "0.849: 1\n",
            "0.851: 1\n",
            "0.853: 1\n",
            "0.855: 1\n",
            "0.857: 1\n",
            "0.859: 1\n",
            "0.861: 1\n",
            "0.863: 1\n",
            "0.865: 1\n",
            "0.867: 1\n",
            "0.869: 1\n",
            "0.871: 1\n",
            "0.873: 1\n",
            "0.875: 1\n",
            "0.877: 1\n",
            "0.879: 1\n",
            "0.881: 1\n",
            "0.883: 1\n",
            "0.885: 1\n",
            "0.887: 1\n",
            "0.889: 1\n",
            "0.891: 1\n",
            "0.893: 1\n",
            "0.895: 1\n",
            "0.897: 1\n",
            "0.899: 1\n",
            "0.901: 1\n",
            "0.903: 1\n",
            "0.905: 1\n",
            "0.907: 1\n",
            "0.909: 1\n",
            "0.911: 1\n",
            "0.913: 1\n",
            "0.915: 1\n",
            "0.917: 1\n",
            "0.919: 1\n",
            "0.921: 1\n",
            "0.923: 1\n",
            "0.925: 1\n",
            "0.927: 1\n",
            "0.929: 1\n",
            "0.931: 1\n",
            "0.933: 1\n",
            "0.935: 1\n",
            "0.937: 1\n",
            "0.9390000000000001: 1\n",
            "0.9410000000000001: 1\n",
            "0.9430000000000001: 1\n",
            "0.9450000000000001: 1\n",
            "0.9470000000000001: 1\n",
            "0.9490000000000001: 1\n",
            "0.9510000000000001: 1\n",
            "0.9530000000000001: 1\n",
            "0.9550000000000001: 1\n",
            "0.9570000000000001: 1\n",
            "0.9590000000000001: 1\n",
            "0.961: 1\n",
            "0.963: 1\n",
            "0.965: 1\n",
            "0.967: 1\n",
            "0.969: 1\n",
            "0.971: 1\n",
            "0.973: 1\n",
            "0.975: 1\n",
            "0.977: 1\n",
            "0.979: 1\n",
            "0.981: 1\n",
            "0.983: 1\n",
            "0.985: 1\n",
            "0.987: 1\n",
            "0.989: 1\n",
            "0.991: 1\n",
            "0.993: 1\n",
            "0.995: 1\n",
            "0.997: 1\n",
            "0.999: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code loops through a range of eps values between 0.001 and 0.05 in increments of 0.002 using np.arange().\n",
        "\n",
        "For each value of eps, it prints the value of eps followed by the number of unique clusters produced by DBSCAN using that value of eps.\n",
        "\n",
        "This information can be used to determine the optimal value of eps to use for clustering the preprocessed descriptions. Generally, a good value of eps is one that produces a moderate number of clusters that are neither too large nor too small."
      ],
      "metadata": {
        "id": "Bqkc9nJFvIUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.029, min_samples=3, metric='cosine').fit(vectors)"
      ],
      "metadata": {
        "id": "x0XNCp9amFFi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This line of code performs clustering using the DBSCAN algorithm with the following parameters:\n",
        "\n",
        "eps: the radius of the neighborhood around each data point. Here, it is set to 0.015.\n",
        "min_samples: the minimum number of points required to form a dense region. Here, it is set to 5.\n",
        "metric: the distance metric to use. Here, it is set to cosine similarity.\n",
        "The input data for clustering are the sentence vectors, vectors, which were computed using the preprocessed descriptions. The clustering result is stored in the dbscan variable, which contains the cluster labels assigned to each sentence."
      ],
      "metadata": {
        "id": "sI8cCtbLvn1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'desc': processed_descriptions, \n",
        "    'label': dbscan.labels_\n",
        "})"
      ],
      "metadata": {
        "id": "TGgPdnzsmGs9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code creates a Pandas DataFrame named results with two columns:\n",
        "\n",
        "-desc: the preprocessed descriptions, which were previously stored in the processed_descriptions list.\n",
        "-label: the cluster labels assigned to each description by DBSCAN, which were stored in the dbscan.labels_ attribute.\n",
        "\n",
        "Each row in the DataFrame represents a single description and its corresponding cluster label."
      ],
      "metadata": {
        "id": "qqcnuXyrwF7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovIROfNtmIIx",
        "outputId": "55751e5c-8085-4a3e-d277-e3f3f86d0400"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    1946\n",
              "-1     179\n",
              " 1       3\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code returns a Pandas Series containing the count of unique values in the 'label' column of the DataFrame results.\n",
        "\n",
        "Since DBSCAN assigns -1 as the label to noise points that are not assigned to any cluster, the value counts will include the count of such noise points as well.\n",
        "\n",
        "The count of each label indicates the number of descriptions that belong to that cluster."
      ],
      "metadata": {
        "id": "LjoyVaAAwmzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in results[results['label'] == 3].index:\n",
        "    print(results.loc[index]['desc'])\n",
        "    print('....')"
      ],
      "metadata": {
        "id": "ErXEaWFpmJ2a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prints out the preprocessed descriptions belonging to cluster label 3.\n",
        "\n",
        "The code first filters the results DataFrame for rows where the 'label' column is equal to 3, using boolean indexing.\n",
        "\n",
        "Then, for each row in the filtered DataFrame, it prints the corresponding description by extracting the 'desc' value using the .loc method, and prints a separator '....' between each description to make it easier to distinguish between them."
      ],
      "metadata": {
        "id": "3RIv-BXJxcy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eps=0.0165 and min_samples=4**"
      ],
      "metadata": {
        "id": "Kytcj0iBjnvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_results = {}\n",
        "for i in tqdm(np.arange(0.001, 1, 0.002)):\n",
        "    dbscan = DBSCAN(eps=i, min_samples=4, metric='cosine').fit(vectors)\n",
        "    labels_results[i] = len(pd.Series(dbscan.labels_).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaP1LNe2hnex",
        "outputId": "10a09ccd-c8ec-4ace-e1c0-8bfa7c12203e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:13<00:00,  6.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(0.001, 0.1, 0.002):\n",
        "    print('{}: {}'.format(i, labels_results[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSofM1-hwi1",
        "outputId": "cc2c1724-2935-48bb-c0c3-98ce553d52d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001: 1\n",
            "0.003: 1\n",
            "0.005: 1\n",
            "0.007: 1\n",
            "0.009000000000000001: 1\n",
            "0.011: 6\n",
            "0.013000000000000001: 13\n",
            "0.015: 11\n",
            "0.017: 3\n",
            "0.019000000000000003: 2\n",
            "0.021: 3\n",
            "0.023: 2\n",
            "0.025: 3\n",
            "0.027000000000000003: 2\n",
            "0.029: 2\n",
            "0.031: 2\n",
            "0.033: 2\n",
            "0.035: 2\n",
            "0.037000000000000005: 2\n",
            "0.039: 2\n",
            "0.041: 2\n",
            "0.043000000000000003: 2\n",
            "0.045: 2\n",
            "0.047: 2\n",
            "0.049: 2\n",
            "0.051000000000000004: 2\n",
            "0.053000000000000005: 2\n",
            "0.055: 2\n",
            "0.057: 2\n",
            "0.059000000000000004: 2\n",
            "0.061: 2\n",
            "0.063: 2\n",
            "0.065: 2\n",
            "0.067: 2\n",
            "0.069: 2\n",
            "0.07100000000000001: 2\n",
            "0.07300000000000001: 2\n",
            "0.075: 2\n",
            "0.077: 2\n",
            "0.079: 2\n",
            "0.081: 2\n",
            "0.083: 2\n",
            "0.085: 2\n",
            "0.08700000000000001: 2\n",
            "0.089: 2\n",
            "0.091: 2\n",
            "0.093: 2\n",
            "0.095: 2\n",
            "0.097: 2\n",
            "0.099: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.0165, min_samples=4, metric='cosine').fit(vectors)"
      ],
      "metadata": {
        "id": "dl_PFdRZh5pL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'desc': processed_descriptions, \n",
        "    'label': dbscan.labels_\n",
        "})"
      ],
      "metadata": {
        "id": "DcrL7tZwh9ZE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZD-urA2h_ZM",
        "outputId": "469fe6b4-a230-414b-f0fe-92080a920870"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    1377\n",
              " 0     743\n",
              " 1       4\n",
              " 2       4\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eps=0.015 and min_samples=5**"
      ],
      "metadata": {
        "id": "bbLxZUNAjc9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_results = {}\n",
        "for i in tqdm(np.arange(0.001, 1, 0.002)):\n",
        "    dbscan = DBSCAN(eps=i, min_samples=5, metric='cosine').fit(vectors)\n",
        "    labels_results[i] = len(pd.Series(dbscan.labels_).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAucEgR6irfT",
        "outputId": "d2dd46de-8efb-493b-96d4-b6a8a42e1610"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:13<00:00,  6.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(0.001, 0.1, 0.002):\n",
        "    print('{}: {}'.format(i, labels_results[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoakEUHXiwWO",
        "outputId": "e1774ac9-52bb-4a32-b231-cc9954fd5802"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001: 1\n",
            "0.003: 1\n",
            "0.005: 1\n",
            "0.007: 1\n",
            "0.009000000000000001: 1\n",
            "0.011: 4\n",
            "0.013000000000000001: 8\n",
            "0.015: 4\n",
            "0.017: 2\n",
            "0.019000000000000003: 2\n",
            "0.021: 2\n",
            "0.023: 2\n",
            "0.025: 2\n",
            "0.027000000000000003: 2\n",
            "0.029: 2\n",
            "0.031: 2\n",
            "0.033: 2\n",
            "0.035: 2\n",
            "0.037000000000000005: 2\n",
            "0.039: 2\n",
            "0.041: 2\n",
            "0.043000000000000003: 2\n",
            "0.045: 2\n",
            "0.047: 2\n",
            "0.049: 2\n",
            "0.051000000000000004: 2\n",
            "0.053000000000000005: 2\n",
            "0.055: 2\n",
            "0.057: 2\n",
            "0.059000000000000004: 2\n",
            "0.061: 2\n",
            "0.063: 2\n",
            "0.065: 2\n",
            "0.067: 2\n",
            "0.069: 2\n",
            "0.07100000000000001: 2\n",
            "0.07300000000000001: 2\n",
            "0.075: 2\n",
            "0.077: 2\n",
            "0.079: 2\n",
            "0.081: 2\n",
            "0.083: 2\n",
            "0.085: 2\n",
            "0.08700000000000001: 2\n",
            "0.089: 2\n",
            "0.091: 2\n",
            "0.093: 2\n",
            "0.095: 2\n",
            "0.097: 2\n",
            "0.099: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.015, min_samples=5, metric='cosine').fit(vectors)"
      ],
      "metadata": {
        "id": "g3S5hGN3i1d4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'desc': processed_descriptions, \n",
        "    'label': dbscan.labels_\n",
        "})"
      ],
      "metadata": {
        "id": "0MouJIuei1n9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jak9PV2gi9jB",
        "outputId": "59d89a01-bd74-478c-8bcc-01e03acae79e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    1712\n",
              " 0     399\n",
              " 1      12\n",
              " 2       5\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}